{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed MPC for cooperation scheme using artificial references.\n",
    "\n",
    "The goal is to implement formation control for a multi-agents system.\n",
    "\n",
    "$m$ agents with dynamics $x_i(t+1) = f(x_i(t), u_i(t))$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a sequential scheme as outlined in the corresponding manuscript:\n",
    "\n",
    ">M. Köhler, M. A. Müller, and F. Allgöwer, \"Distributed MPC for Self-Organized Cooperation of Multi-Agent Systems,\", 2023, available on arxiv. doi: 10.48550/arXiv.2210.10128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkmpc\n",
    "import numpy as np\n",
    "import casadi as cas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the system.\n",
    "We use the 10 state quadcopter system taken from\n",
    ">Hu, Haimin / Feng, Xuhui / Quirynen, Rien / Villanueva, Mario Eduardo / Houska, Boris \n",
    ">Real-Time Tube MPC Applied to a 10-State Quadrotor Model \n",
    ">2018\n",
    ">2018 Annual American Control Conference (ACC) \n",
    "\n",
    "We use the Euler method to discretise the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3 # Set number of agents.\n",
    "state_dims = [10, 10, 10, 10, 10]  # Set the state dimensions.\n",
    "input_dims = [3, 3, 3, 3, 3]  # Set the input dimensions.\n",
    "output_dim = 3  # Set the output dimension, which is the same for all agents.\n",
    "\n",
    "disc_step_size = 0.1  # Set step size for discretisation.\n",
    "grav = 9.81  # Set the gravity of Earth.\n",
    "\n",
    "# Specifiy initial states.\n",
    "initial_state_list = [np.array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]),\n",
    "                      np.array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]),\n",
    "                      np.array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]),\n",
    "                      np.array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]),\n",
    "                      np.array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]])]\n",
    "\n",
    "dynamics_list = []  # collect dynamic functions\n",
    "output_maps = []  # collect output functions\n",
    "for i in range(num_agents):\n",
    "    x = cas.MX.sym('x', state_dims[i])  # define a symbolic state\n",
    "    u = cas.MX.sym('u', input_dims[i])  # define a symbolic input\n",
    "    \n",
    "    dynamics_list.append(cas.Function('dynamics', [x,u],\n",
    "                                      [cas.vertcat(x[0] + disc_step_size*x[3],\n",
    "                                       x[1] + disc_step_size*x[4],\n",
    "                                       x[2] + disc_step_size*x[5],\n",
    "                                       x[3] + disc_step_size*grav*cas.tan(x[6]),\n",
    "                                       x[4] + disc_step_size*grav*cas.tan(x[7]),\n",
    "                                       x[5] + disc_step_size*(-grav + 0.91*u[2]),\n",
    "                                       x[6] + disc_step_size*(-8*x[6] + x[8]),\n",
    "                                       x[7] + disc_step_size*(-8*x[7] + x[9]),\n",
    "                                       x[8] + disc_step_size*10*(-x[6] + u[0]),\n",
    "                                       x[9] + disc_step_size*10*(-x[7] + u[1])\n",
    "                                       )],\n",
    "                                      ))  # define the dynamics\n",
    "    output_maps.append(cas.Function('output', [x,u], [cas.vertcat(x[0], x[1], x[2])], ['x', 'u'], ['y']))  # first three states are the output\n",
    "    \n",
    "# Define the agents.\n",
    "agents = []\n",
    "for i in range(num_agents):\n",
    "    # Initialise the agent.\n",
    "    agents.append(mkmpc.agent(id = str(i+1), state_dim = state_dims[i], input_dim = input_dims[i],\n",
    "                              dynamics = dynamics_list[i], initial_time=0, initial_state=initial_state_list[i],\n",
    "                              output_map=output_maps[i], output_dim=output_dim))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the same constraints for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constraints for each agent.\n",
    "box_state_constr = np.array([[-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10]])\n",
    "\n",
    "for agent in agents:\n",
    "    agent.set_constraints(box_state_constraints=box_state_constr)  # set state constraints.\n",
    "    agent.input_constraints[\"A\"] = np.array([[1, 0, 0],\n",
    "                                             [-1, 0, 0],\n",
    "                                             [0, 1, 0],\n",
    "                                             [0, -1, 0],\n",
    "                                             [0, 0, 1],\n",
    "                                             [0, 0, -1]])\n",
    "    agent.input_constraints[\"b\"] = np.array([[np.pi/9],\n",
    "                                             [np.pi/9],\n",
    "                                             [np.pi/9],\n",
    "                                             [np.pi/9],\n",
    "                                             [2*grav],\n",
    "                                             [0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set neighbours as\n",
    "* $\\mathcal{N}_1 = \\{2, 5\\}$\n",
    "* $\\mathcal{N}_2 = \\{1, 3\\}$\n",
    "* $\\mathcal{N}_3 = \\{2, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set neighbours in lists as described above. Note that the index starts at 0.\n",
    "agents[0].neighbours = [agents[1], agents[2]]\n",
    "agents[1].neighbours = [agents[0], agents[2]]\n",
    "agents[2].neighbours = [agents[1], agents[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the stage cost for tracking, i.e. $l_i = \\Vert x_i - x_{\\mathrm{c},i} \\Vert^2{Q_i} + \\Vert u_i - u_{\\mathrm{c},i} \\Vert^2_{R_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_agents):\n",
    "    # Define the artificial equilibrium. \n",
    "    x = cas.MX.sym('x', state_dims[i])\n",
    "    u = cas.MX.sym('u', input_dims[i])\n",
    "    xc = cas.MX.sym('x_c', state_dims[i])\n",
    "    uc = cas.MX.sym('u_c', input_dims[i])\n",
    "    \n",
    "    # Set the weight for the distance of the state to the equilibrium.\n",
    "    Q = np.eye(state_dims[i])\n",
    "    # Set the weight for the distance of the input to the equilibrium.\n",
    "    R = np.eye(input_dims[i])\n",
    "    \n",
    "    stage_cost = cas.Function('stage_cost', [x, u, xc, uc], [ (x - xc).T@Q@(x - xc) + (u - uc).T@R@(u - uc) ],\n",
    "                              ['x', 'u', 'xc', 'uc'], ['l'])\n",
    "    \n",
    "    # Add stage cost to agents.\n",
    "    agents[i].stage_cost = stage_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost for cooperation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc1 = cas.MX.sym('yc1', output_dim)\n",
    "yc2 = cas.MX.sym('yc2', output_dim)\n",
    "\n",
    "dij = 1  # Define the distance each agent should have.\n",
    "coop_weight = 1  # Define a weight for the cooperation cost.\n",
    "\n",
    "# consensus and formation:\n",
    "bilat_coop_cost = cas.Function('cooperation_cost', [yc1, yc2], [ coop_weight*( ( ((yc1[0:2] - yc2[0:2]).T@(yc1[0:2] - yc2[0:2])) - dij**2)**2 + (yc1[2] - yc2[2]).T@(yc1[2] - yc2[2]) )], ['yc1', 'yc2'], ['V_ij^c'])\n",
    "\n",
    "# Set the bilateral cooperation cost for each agent.\n",
    "for agent in agents:\n",
    "    agent.bilat_coop_cost = bilat_coop_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an easy set of admissible cooperation outputs $\\mathcal{Y}_i$, the same for all agents.\n",
    "\n",
    "* $\\mathcal{Y}_i = [-8, 8]^3 \\cap \\{y_i \\mid y_{i,3} \\ge 0\\}$ $\\forall i$\n",
    "\n",
    "Define the sets in the form $A y_i^\\mathrm{c} \\le b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the A matrix of the constraint inequality which is the same for all agents.\n",
    "for agent in agents:\n",
    "    agent.cooperation_output_constraint = {}\n",
    "    agent.cooperation_output_constraint[\"A\"] = np.array([[1, 0, 0],\n",
    "                                                         [-1, 0, 0],\n",
    "                                                         [0, 1, 0],\n",
    "                                                         [0, -1, 0],\n",
    "                                                         [0, 0, 1],\n",
    "                                                         [0, 0, -1]])\n",
    "    agent.cooperation_output_constraint[\"b\"] = np.array([[8],\n",
    "                                                         [8],\n",
    "                                                         [8],\n",
    "                                                         [8],\n",
    "                                                         [8],\n",
    "                                                         [0]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the sequential MPC scheme."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set simulation and MPC parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_simulation_time = 600\n",
    "horizon = 20\n",
    "print('Horizon is', horizon*disc_step_size, 'seconds.')\n",
    "print('Simulation time is', last_simulation_time*disc_step_size, 'seconds.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of all agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all cooperation outputs to zero for the initialisation since 0 is an admissible cooperation output for all agents.\n",
    "for agent in agents:\n",
    "    agent.current_cooperation_output = np.zeros((agent.output_dim, 1))\n",
    "\n",
    "# Set the initial state.\n",
    "# agents[0].current_state = np.array([[0.00], [0.00], [1], [0], [0], [0], [0], [0], [0], [0]])\n",
    "# agents[1].current_state = np.array([[0.00], [0.00], [2], [0], [0], [0], [0], [0], [0], [0]])\n",
    "# agents[2].current_state = np.array([[0.00], [0.00], [3], [0], [0], [0], [0], [0], [0], [0]])\n",
    "\n",
    "agents[0].current_state = np.array([[1e-5], [0.00], [1], [0], [0], [0], [0], [0], [0], [0]])\n",
    "agents[1].current_state = np.array([[-1e-5], [1e-5], [2], [0], [0], [0], [0], [0], [0], [0]])\n",
    "agents[2].current_state = np.array([[-1e-5], [-1e-5], [3], [0], [0], [0], [0], [0], [0], [0]])\n",
    "\n",
    "# Take output given by initial state as first cooperation output.\n",
    "for agent in agents:\n",
    "    agent.current_cooperation_output = np.copy(agent.current_state[0:3])\n",
    "\n",
    "# Solve an initial optimisation problem.\n",
    "# This is done only to get a feel for the parameters, including the initial conditions, and has no effect on the actual scheme simulated below.\n",
    "for agent in agents:\n",
    "    warm_start = []\n",
    "    # Add warm start of the input trajectory.\n",
    "    for i in range(horizon):\n",
    "        warm_start.append(np.array([[0], [0], [9.81/0.91]]))\n",
    "    # Add warm start of the state trajectory.\n",
    "    for i in range(horizon+1):\n",
    "        warm_start.append(agent.current_state)\n",
    "    # Add warm start of the cooperation input.\n",
    "    warm_start.append(np.array([[0], [0], [9.81/0.91]]))\n",
    "    # Add warm start of the cooperation state.\n",
    "    warm_start.append(agent.current_state)\n",
    "    # Add warm start of the cooperation output.\n",
    "    warm_start.append(agent.current_state[0:3])\n",
    "    warm_start = np.concatenate(warm_start)\n",
    "    \n",
    "    grad_size = 0\n",
    "    saved_neighbours_cooperation_outputs = []\n",
    "    for neighbour in agent.neighbours:\n",
    "        #grad_size += np.linalg.norm(neighbour.current_cooperation_output[0:2] - agent.current_cooperation_output[0:2])\n",
    "        grad_size += np.linalg.norm(neighbour.current_cooperation_output - agent.current_cooperation_output)\n",
    "    if np.abs(grad_size) < 1e-8:\n",
    "        # Perturb cooperation output of neighbours.\n",
    "        for neighbour in agent.neighbours:\n",
    "            print(neighbour.current_cooperation_output)\n",
    "            # Save the cooperation output of the neighbour.\n",
    "            saved_neighbours_cooperation_outputs.append(np.copy(neighbour.current_cooperation_output))\n",
    "            # Perturb cooperation output of neighbour.\n",
    "            if neighbour.id == '1':\n",
    "                pass\n",
    "                neighbour.current_cooperation_output = neighbour.current_cooperation_output + np.array([[0.01], [0], [0]])    \n",
    "            elif neighbour.id == '2':\n",
    "                pass\n",
    "                neighbour.current_cooperation_output = neighbour.current_cooperation_output + np.array([[-0.01], [0.01], [0]])    \n",
    "            elif neighbour.id == '3':\n",
    "                pass\n",
    "                neighbour.current_cooperation_output = neighbour.current_cooperation_output + np.array([[0.02], [-0.01], [0]])    \n",
    "        # Generate the warm start and use it instead.\n",
    "        warm_start_sol = mkmpc.MPC_for_cooperation(agent, horizon=horizon, warm_start=warm_start)\n",
    "        warm_start = []\n",
    "        # Append warm start of the input trajectoy by taking the old one, shifting it and appending the currently optimal equilibrium's input.\n",
    "        for i in range(horizon-1):\n",
    "                warm_start.append(warm_start_sol[\"u_opt\"][0:agent.input_dim, i+1:i+2])\n",
    "        warm_start.append(warm_start_sol[\"uc_opt\"])\n",
    "        # Append warm start of the state trajectory by taking the old one, shifting it and appending the currently optimal equilibrium's state.\n",
    "        for i in range(horizon):\n",
    "                warm_start.append(warm_start_sol[\"x_opt\"][0:agent.state_dim, i+1:i+2])\n",
    "        warm_start.append(warm_start_sol[\"xc_opt\"])\n",
    "\n",
    "        # Append warm start of cooperation input.\n",
    "        warm_start.append(warm_start_sol[\"uc_opt\"])\n",
    "        # Append warm start of the cooperation state.\n",
    "        warm_start.append(warm_start_sol[\"xc_opt\"])\n",
    "        \n",
    "        # Append the warm start of the cooperation output.\n",
    "        warm_start.append(warm_start_sol[\"yc_opt\"])\n",
    "        \n",
    "        warm_start = np.concatenate(warm_start)\n",
    "        \n",
    "        # Restore cooperation outputs of the neighbours.\n",
    "        for neighbour in agent.neighbours:\n",
    "            neighbour.current_cooperation_output = np.copy(saved_neighbours_cooperation_outputs[agent.neighbours.index(neighbour)])\n",
    "\n",
    "    agent.current_MPC_sol = mkmpc.MPC_for_cooperation(agent, horizon=horizon, warm_start=warm_start)\n",
    "    agent.current_cooperation_output = np.copy(agent.current_MPC_sol[\"yc_opt\"])\n",
    "\n",
    "# Plot initial solution.\n",
    "fig_init1, ax_init1 = plt.subplots()\n",
    "fig_init2, ax_init2 = plt.subplots()\n",
    "fig_init3, ax_init3 = plt.subplots()\n",
    "for agent in agents:\n",
    "    #fig, ax = plt.subplots()  # Uncomment to show individual plots.\n",
    "    time_steps = range(0, horizon+1)\n",
    "    ax_init1.plot(time_steps, agent.current_MPC_sol[\"x_opt\"][0,:])\n",
    "    ax_init1.scatter(horizon, agent.current_MPC_sol[\"yc_opt\"][0], marker=\"x\")\n",
    "    ax_init1.grid(True)\n",
    "    ax_init2.plot(time_steps, agent.current_MPC_sol[\"x_opt\"][1,:])\n",
    "    ax_init2.scatter(horizon, agent.current_MPC_sol[\"yc_opt\"][1], marker=\"x\")\n",
    "    ax_init2.grid(True)\n",
    "    ax_init3.plot(time_steps, agent.current_MPC_sol[\"x_opt\"][2,:])\n",
    "    ax_init3.scatter(horizon, agent.current_MPC_sol[\"yc_opt\"][2], marker=\"x\")\n",
    "    ax_init3.grid(True)\n",
    "\n",
    "# Take output given by initial state as first cooperation output.\n",
    "for agent in agents:\n",
    "    agent.current_cooperation_output = np.copy(agent.current_state[0:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the closed-loop system.\n",
    "closed_loop_evolution = []\n",
    "\n",
    "for t in range(last_simulation_time+1):\n",
    "    # Track the time.\n",
    "    closed_loop_evolution.append([None]*len(agents))\n",
    "    \n",
    "    # Go in sequence over the agents.\n",
    "    for agent in agents:\n",
    "        closed_loop_evolution[t][agents.index(agent)] = {\"time\":t}\n",
    "        agent.current_time = t\n",
    "        # Keep track of the current state.\n",
    "        closed_loop_evolution[t][agents.index(agent)].update({\"current_state\":np.copy(agent.current_state)})\n",
    "        \n",
    "        # Generate a warm start. Decision vector: (u, x, uc, xc, yc)\n",
    "        if t == 0:\n",
    "            #warm_start = np.zeros((u.shape[0] + x.shape[0] + uc.shape[0] + xc.shape[0] + yc.shape[0], 1))\n",
    "            warm_start = []\n",
    "            # Add warm start of the input trajectory.\n",
    "            for i in range(horizon):\n",
    "                warm_start.append(np.array([[0], [0], [9.81/0.91]]))\n",
    "            # Add warm start of the state trajectory.\n",
    "            for i in range(horizon+1):\n",
    "                warm_start.append(agent.current_state)\n",
    "            # Add warm start of the cooperation input.\n",
    "            warm_start.append(np.array([[0], [0], [9.81/0.91]]))\n",
    "            # Add warm start of the cooperation state.\n",
    "            warm_start.append(agent.current_state)\n",
    "            # Add warm start of the cooperation output.\n",
    "            warm_start.append(agent.current_state[0:3])\n",
    "            warm_start = np.concatenate(warm_start)\n",
    "        else:\n",
    "            warm_start = []\n",
    "            # Append warm start of the input trajectoy by taking the old one, shifting it and appending the currently optimal equilibrium's input.\n",
    "            for i in range(horizon-1):\n",
    "                 warm_start.append(agent.current_MPC_sol[\"u_opt\"][0:agent.input_dim, i+1:i+2])\n",
    "            warm_start.append(agent.current_MPC_sol[\"uc_opt\"])\n",
    "            # Append warm start of the state trajectory by taking the old one, shifting it and appending the currently optimal equilibrium's state.\n",
    "            for i in range(horizon):\n",
    "                 warm_start.append(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, i+1:i+2])\n",
    "            warm_start.append(agent.current_MPC_sol[\"xc_opt\"])\n",
    "\n",
    "            # Append warm start of cooperation input.\n",
    "            warm_start.append(agent.current_MPC_sol[\"uc_opt\"])\n",
    "            # Append warm start of the cooperation state.\n",
    "            warm_start.append(agent.current_MPC_sol[\"xc_opt\"])\n",
    "            \n",
    "            # Append the warm start of the cooperation output.\n",
    "            warm_start.append(agent.current_MPC_sol[\"yc_opt\"])\n",
    "            \n",
    "            warm_start = np.concatenate(warm_start)\n",
    "            \n",
    "        # Check if gradient w.r.t. formation goal is zero, e.g. all agents are at the same position.\n",
    "        # If the gradient is zero, perturb the position of the neighbour to provide a suitable warm start for the solver.\n",
    "        # Note that this is unlikely to arrise in practise expect if the initial condition is chosen badly as done in the paper (on purpose).\n",
    "        grad_size_approx = 0\n",
    "        saved_neighbours_cooperation_outputs = []\n",
    "        for neighbour in agent.neighbours:\n",
    "            grad_size_approx += np.linalg.norm(neighbour.current_cooperation_output[0:2] - agent.current_cooperation_output[0:2])\n",
    "        if np.abs(grad_size_approx) < 1e-8:\n",
    "            # Perturb cooperation output of neighbours.\n",
    "            for neighbour in agent.neighbours:\n",
    "                neighbours_perturbed = 1 # Set a flag to later restore the cooperation outputs of the neighbours.\n",
    "                # Save the cooperation output of the neighbour.\n",
    "                saved_neighbours_cooperation_outputs.append(np.copy(neighbour.current_cooperation_output))\n",
    "                # Perturb cooperation output of neighbour.\n",
    "                if neighbour.id == '1':\n",
    "                    neighbour.current_cooperation_output = neighbour.current_cooperation_output + np.array([[0.01], [0], [0]])    \n",
    "                elif neighbour.id == '2':\n",
    "                    neighbour.current_cooperation_output = neighbour.current_cooperation_output + np.array([[-0.01], [0.01], [0]])    \n",
    "                elif neighbour.id == '3':\n",
    "                    neighbour.current_cooperation_output = neighbour.current_cooperation_output + np.array([[0.02], [-0.01], [0]])    \n",
    "            # Generate the warm start and use it instead.\n",
    "            warm_start_sol = mkmpc.MPC_for_cooperation(agent, horizon=horizon, warm_start=warm_start)\n",
    "            warm_start = []\n",
    "            # Append warm start of the input trajectoy by taking the old one, shifting it and appending the currently optimal equilibrium's input.\n",
    "            for i in range(horizon-1):\n",
    "                 warm_start.append(warm_start_sol[\"u_opt\"][0:agent.input_dim, i+1:i+2])\n",
    "            warm_start.append(warm_start_sol[\"uc_opt\"])\n",
    "            # Append warm start of the state trajectory by taking the old one, shifting it and appending the currently optimal equilibrium's state.\n",
    "            for i in range(horizon):\n",
    "                 warm_start.append(warm_start_sol[\"x_opt\"][0:agent.state_dim, i+1:i+2])\n",
    "            warm_start.append(warm_start_sol[\"xc_opt\"])\n",
    "\n",
    "            # Append warm start of cooperation input.\n",
    "            warm_start.append(warm_start_sol[\"uc_opt\"])\n",
    "            # Append warm start of the cooperation state.\n",
    "            warm_start.append(warm_start_sol[\"xc_opt\"])\n",
    "            \n",
    "            # Append the warm start of the cooperation output.\n",
    "            warm_start.append(warm_start_sol[\"yc_opt\"])\n",
    "            \n",
    "            warm_start = np.concatenate(warm_start)\n",
    "            \n",
    "            # Restore cooperation outputs of the neighbours.\n",
    "            for neighbour in agent.neighbours:\n",
    "                neighbour.current_cooperation_output = np.copy(saved_neighbours_cooperation_outputs[agent.neighbours.index(neighbour)])\n",
    "        \n",
    "        # Solve the MPC problem.\n",
    "        agent.current_MPC_sol = mkmpc.MPC_for_cooperation(agent, horizon=horizon, warm_start=warm_start)\n",
    "        # Keep track of the solution.\n",
    "        closed_loop_evolution[t][agents.index(agent)].update(agent.current_MPC_sol)\n",
    "        # Update the current state of the agent. \n",
    "        # Without model errors and with satisfaction of the dynamic constraint, the next predicted state will be the next closed-loop state.\n",
    "        agent.current_state = np.copy(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, 1:2])\n",
    "        # Update the cooperation output of the agent.\n",
    "        agent.current_cooperation_output = np.copy(agent.current_MPC_sol[\"yc_opt\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "#### Plot the closed-loop evolution.\n",
    "Note that these plots are not the plots used in the paper, which where created using a different work flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the closed-loop evolution.\n",
    "time_steps = range(0, last_simulation_time+1)\n",
    "\n",
    "# Create a vector with time in seconds.\n",
    "time = []\n",
    "for t in time_steps:\n",
    "    time.append(t*disc_step_size)\n",
    "    \n",
    "fig1_states, (ax_states_1, ax_states_2, ax_states_3) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(20,6))\n",
    "fig2_states, (ax_states_4, ax_states_5, ax_states_6) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(20,6))\n",
    "fig3_states, (ax_states_7, ax_states_8, ax_states_9, ax_states_10) = plt.subplots(nrows=1, ncols=4, sharex=True, figsize=(20,6))\n",
    "\n",
    "for ax_i in fig1_states.axes:\n",
    "    ax_i.grid(True)\n",
    "for ax_i in fig2_states.axes:\n",
    "    ax_i.grid(True)\n",
    "for ax_i in fig3_states.axes:\n",
    "    ax_i.grid(True)\n",
    "    \n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.grid(True)\n",
    "ax3 = plt.figure().add_subplot(projection='3d')\n",
    "ax3.grid(True)\n",
    "\n",
    "for agent in agents:\n",
    "    # Extract state evolution of agent.\n",
    "    agent_state_evo = []\n",
    "    for t in time_steps:\n",
    "        agent_state_evo.append(closed_loop_evolution[t][agents.index(agent)][\"current_state\"])\n",
    "    # Build state evolution matrix.\n",
    "    state_evo_mat = np.concatenate(agent_state_evo, axis=1)\n",
    "    \n",
    "    # Extract input evolution of agent.\n",
    "    agent_input_evo = []\n",
    "    for t in time_steps:\n",
    "        agent_input_evo.append(closed_loop_evolution[t][agents.index(agent)][\"u_opt\"][0:agent.input_dim, 0:1])\n",
    "    input_evo_mat = np.concatenate(agent_input_evo, axis=1)\n",
    "    \n",
    "    label_str = \"Agent \" + agent.id  # For labelling the plot.\n",
    "    \n",
    "    ax_states_1.plot(time, state_evo_mat[0, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_1.set_ylabel('pos1')\n",
    "    ax_states_1.set_xlabel('time in s')\n",
    "    \n",
    "    ax_states_2.plot(time, state_evo_mat[1, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_2.set_ylabel('pos2')\n",
    "    ax_states_2.set_xlabel('time in s')\n",
    "    \n",
    "    # Plot evolution of the altitute.\n",
    "    ax_states_3.plot(time, state_evo_mat[2, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_3.set_ylabel('altitute in m')\n",
    "    ax_states_3.set_xlabel('time in s')\n",
    "    \n",
    "    ax_states_4.plot(time, state_evo_mat[3, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_4.set_ylabel('x_4')\n",
    "    ax_states_4.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_5.plot(time, state_evo_mat[4, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_5.set_ylabel('x_5')\n",
    "    ax_states_5.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_6.plot(time, state_evo_mat[5, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_6.set_ylabel('x_6')\n",
    "    ax_states_6.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_7.plot(time, state_evo_mat[6, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_7.set_ylabel('x_7')\n",
    "    ax_states_7.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_8.plot(time, state_evo_mat[7, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_8.set_ylabel('x_8')\n",
    "    ax_states_8.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_9.plot(time, state_evo_mat[8, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_9.set_ylabel('x_9')\n",
    "    ax_states_9.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_10.plot(time, state_evo_mat[9, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_10.set_ylabel('x_10')\n",
    "    ax_states_10.set_xlabel('time in seconds')\n",
    "    \n",
    "    # Plot 2D evolution of position without altitute.\n",
    "    ax2.plot(state_evo_mat[0, :], state_evo_mat[1, :], label=label_str, marker='x', markersize=0)\n",
    "    \n",
    "    # Plot 3D evolution of position.\n",
    "    ax3.plot(state_evo_mat[0, :], state_evo_mat[1, :], state_evo_mat[2, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
